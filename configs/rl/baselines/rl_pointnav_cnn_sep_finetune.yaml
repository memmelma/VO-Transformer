# BASE_TASK_CONFIG_PATH: "configs/point_nav_habitat_challenge_2020.yaml"
BASE_TASK_CONFIG_PATH: "configs/challenge_pointnav2021.local.rgbd.yaml"

# training settings
ENGINE_NAME: "efficient_ddppo"
ENV_NAME: "NavRLEnv"
SENSORS: ["DEPTH_SENSOR", "RGB_SENSOR"]
NUM_UPDATES: 2500
NUM_PROCESSES: 16
SIMULATOR_GPU_ID: 0
TORCH_GPU_ID: 1
N_GPUS: -1

# logging settings
LOG_DIR: "train_log/rl/vit/"
LOG_FILE: "{{LOG_DIR}}/train.log"
INFO_DIR: "{{LOG_DIR}}/infos"
CHECKPOINT_FOLDER: "{{LOG_DIR}}/checkpoints"
TENSORBOARD_DIR: "{{LOG_DIR}}/tb"
VIDEO_OPTION: ["wandb"]   # choices: ["none", "disk", "tensorboard", "wandb"], "none" is used for storing image info but not generating video
VIDEO_DIR: "{{LOG_DIR}}/videos"
CHECKPOINT_INTERVAL: 50
LOG_INTERVAL: 10

DEBUG: False

# evaluation settings
EVAL:
  SPLIT: val
  TEST_EPISODE_COUNT: -1
  SAVE_RANKED_IMGS: False
  RANK_TOP_K: 1
  RESIZE_TOPDOWN_MAP: False
  DRAW_SHORTEST_PATH: True

  EVAL_WITH_CKPT: True

  # EVAL_CKPT_PATH: "pretrained_ckpts/rl/tune_vo/rl_tune_vo.pth"  # path to the checkpoint you want to evaluate with
  EVAL_CKPT_PATH: "replace with fine-tuned model"  # path to the checkpoint you want to evaluate with


RESUME_TRAIN: False
RESUME_STATE_FILE: "resume_train_ckpt.pth"   # path to the checkpoint you want to resume training from

RL:
  SUCCESS_REWARD: 2.5

  OBS_TRANSFORM: "none"   # choices: ["none", "resize", "resize_crop"]
  VIS_SIZE_W: 341
  VIS_SIZE_H: 192

  TUNE_WITH_VO: True

  Policy:
    name: "resnet_rnn_policy"
    visual_backbone: "resnet18"
    rnn_backbone: "LSTM"
    num_recurrent_layers: 2
    visual_types: ["depth"]

  PPO:
    # ppo params
    clip_param: 0.2
    ppo_epoch: 1
    num_mini_batch: 2
    value_loss_coef: 0.5
    entropy_coef: 0.01
    lr: 1.e-4    # 1e-4 (tune with vo), 2.5e-4 (train from scratch)
    eps: 1e-5
    max_grad_norm: 0.2
    num_steps: 128
    use_gae: True
    gamma: 0.99
    tau: 0.95
    use_linear_clip_decay: False
    use_linear_lr_decay: False
    reward_window_size: 50

    use_normalized_advantage: False

    hidden_size: 512

  DDPPO:
    sync_frac: 0.6
    # The PyTorch distributed backend to use
    # distrib_backend: GLOO
    distrib_backend: NCCL
    # Initialize with pretrained weights
    pretrained: True # False
    # Visual encoder backbone
    pretrained_weights: "/datasets/home/memmel/PointNav-VO/pretrained_ckpts/rl/no_tune/rl_no_tune.pth"
    # Initialize just the visual encoder backbone with pretrained weights
    pretrained_encoder: True # False
    # Whether or not the visual encoder backbone will be trained.
    train_encoder: True # True
    # Whether or not to reset the critic linear layer
    reset_critic: False

VO:
  USE_VO_MODEL: True

  VO_TYPE: "REGRESS"

  OBS_TRANSFORM: "none"
  VIS_SIZE_W: 341
  VIS_SIZE_H: 192

  REGRESS_MODEL:
    name: "vo_cnn_rgb_d_dd_top_down"
    visual_backbone: "resnet50" # 'small', 'base', 'large', 'hybrid'
    train_backbone: True # True: also train the ViT encoder, False: freeze
    pretrain_backbone: 'None' # 'in21k', 'dino' (in1k), 'omnidata', 'None'
    omnidata_model_path: 'dpt/pretrained_models'
    cls_action: False
    hidden_size: 512
    visual_type: ["rgb", "depth", "discretized_depth", "top_down_view"] # choices: ["rgb", "depth", "discretized_depth", "top_down_view"]
    discretize_depth: "hard" # choices: ['none', 'hard']
    discretized_depth_channels: 10
    dropout_p: 0.2

    regress_type: "sep_act"   # choices: ["unified_act", "sep_act"]
    mode: "det"  # choices: ["rnd", "det"]
    rnd_mode_n: 10
    pretrained: True
    pretrained_type: "rgb_d_dd_top_down_inv"
    all_pretrained_ckpt: {
      "rgb_d_dd_top_down_inv": {
        "forward": "train_log/cnn/seed_100-vo-noise_1-train-rgb_d_dd_proj-dd_hard_10-m_cen_1-act_1-model_vo_cnn_rgb_d_dd_top_down-resnet50-geo__inv_w_1-l_mult_fix_1-1.0_1.0_1.0-dpout_0.2-e_150-b_256-lr_0.0002-w_de_0.0-20220311_043217398565/checkpoints/ckpt_epoch_140.pth",
        "left": "train_log/cnn/seed_100-vo-noise_1-train-rgb_d_dd_proj-dd_hard_10-m_cen_1-act_2-model_vo_cnn_rgb_d_dd_top_down-resnet50-geo_inv_aug_inv_w_1-l_mult_fix_1-1.0_1.0_1.0-dpout_0.2-e_150-b_256-lr_0.0002-w_de_0.0-20220310_182410420384/checkpoints/ckpt_epoch_140.pth",
        "right": "train_log/cnn/seed_100-vo-noise_1-train-rgb_d_dd_proj-dd_hard_10-m_cen_1-act_3-model_vo_cnn_rgb_d_dd_top_down-resnet50-geo_inv_aug_inv_w_1-l_mult_fix_1-1.0_1.0_1.0-dpout_0.2-e_150-b_256-lr_0.0002-w_de_0.0-20220310_182439129620/checkpoints/ckpt_epoch_140.pth",
      },
    }

    # # choices:
    # # [vo_cnn, vo_cnn_rgb, vo_cnn_wider, vo_cnn_deeper,
    # #  vo_cnn_act_embed, vo_cnn_wider_act_embed,
    # #  vo_cnn_rgb_d_dd, vo_cnn_rgb_d_top_down, vo_cnn_rgb_dd_top_down, vo_cnn_d_dd_top_down,
    # #  vo_cnn_rgb_d_dd_top_down]
    # name: "vo_cnn_rgb_d_dd_top_down"
    # visual_backbone: "resnet18"
    # hidden_size: 512
    # visual_type: ["rgb", "depth", "discretized_depth", "top_down_view"]  # choices: ["rgb", "depth", "discretized_depth", "top_down_view"]
    # dropout_p: 0.2

    # discretize_depth: "hard"  # choices: ['none', 'hard']
    # discretized_depth_channels: 10

    # regress_type: "sep_act"   # choices: ["unified_act", "sep_act"]
    # mode: "det"  # choices: ["rnd", "det"]
    # rnd_mode_n: 10
    # pretrained: True
    # pretrained_type: "rgb_d_dd_top_down_inv_joint"
    # all_pretrained_ckpt: {
    #   "rgb_d_dd_top_down_inv_joint": {
    #     "forward": "pretrained_ckpts/vo/act_forward.pth",
    #     "left": "pretrained_ckpts/vo/act_left_right_inv_joint.pth",
    #     "right": "pretrained_ckpts/vo/act_left_right_inv_joint.pth",
    #   },
    # }

