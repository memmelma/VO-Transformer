BASE_TASK_CONFIG_PATH: "configs/point_nav_habitat_challenge_2020.yaml"

# training settings
ENGINE_NAME: "efficient_ddppo"
ENV_NAME: "NavRLEnv"
SENSORS: ["DEPTH_SENSOR", "RGB_SENSOR"]
NUM_UPDATES: 15000
# max number of processes is 14, the number of evaluation scenes
NUM_PROCESSES: 14
# GPU to use for the simulation processes
SIMULATOR_GPU_ID: 0
# GPU to use for pytorch models
TORCH_GPU_ID: 0
# use 1 GPU for evaluation
N_GPUS: 1

# logging settings
LOG_DIR: "train_log/rl/vit/"
LOG_FILE: "{{LOG_DIR}}/train.log"
INFO_DIR: "{{LOG_DIR}}/infos"
CHECKPOINT_FOLDER: "{{LOG_DIR}}/checkpoints"
TENSORBOARD_DIR: "{{LOG_DIR}}/tb"
# whether to render a video of the navigation
# set to [] to avoid VRAM overflow when videos are not needed
# ["none", "disk", "tensorboard", "wandb"], "none" is used for storing image info but not generating video
VIDEO_OPTION: []
VIDEO_DIR: "{{LOG_DIR}}/videos"
CHECKPOINT_INTERVAL: 50
LOG_INTERVAL: 10

# activate debugging mode
DEBUG: False

# if wandb is setup, define a project here
# else set to ''
WANDB_PROJECT: ''

# evaluation settings
EVAL:
  SPLIT: val
  TEST_EPISODE_COUNT: -1
  SAVE_RANKED_IMGS: False
  RANK_TOP_K: 1
  RESIZE_TOPDOWN_MAP: False
  DRAW_SHORTEST_PATH: True

  EVAL_WITH_CKPT: True

  # path to the RL navigation agent checkpoint to evaluate with
  EVAL_CKPT_PATH: "./pretrained_ckpts/rl/no_tune/rl_no_tune.pth"


RESUME_TRAIN: False
# path to the checkpoint to resume training from
RESUME_STATE_FILE: "resume_train_ckpt.pth"

RL:
  SUCCESS_REWARD: 2.5

  OBS_TRANSFORM: "none"
  VIS_SIZE_W: 341
  VIS_SIZE_H: 192

  TUNE_WITH_VO: True

  Policy:
    name: "resnet_rnn_policy"
    visual_backbone: "resnet18"
    rnn_backbone: "LSTM"
    num_recurrent_layers: 2
    visual_types: ["depth"]

  PPO:
    clip_param: 0.2
    ppo_epoch: 1
    num_mini_batch: 2
    value_loss_coef: 0.5
    entropy_coef: 0.01
    # 1e-4 (tune with vo), 2.5e-4 (train from scratch)
    lr: 1.e-4
    eps: 1e-5
    max_grad_norm: 0.2
    num_steps: 128
    use_gae: True
    gamma: 0.99
    tau: 0.95
    use_linear_clip_decay: False
    use_linear_lr_decay: False
    reward_window_size: 50
    use_normalized_advantage: False
    hidden_size: 512

  DDPPO:
    sync_frac: 0.6
    # The PyTorch distributed backend to use
    # distrib_backend: GLOO
    distrib_backend: NCCL
    # Initialize with pretrained weights
    pretrained: True
    # Visual encoder backbone
    pretrained_weights: "/datasets/home/memmel/PointNav-VO/pretrained_ckpts/rl/no_tune/rl_no_tune.pth"
    # Initialize just the visual encoder backbone with pretrained weights
    pretrained_encoder: True
    # Whether or not the visual encoder backbone will be trained.
    train_encoder: True
    # Whether or not to reset the critic linear layer
    reset_critic: False

VO:

  # whether to drop-in a VO model
  USE_VO_MODEL: True

  VO_TYPE: "REGRESS"

  OBS_TRANSFORM: "none"
  VIS_SIZE_W: 341
  VIS_SIZE_H: 192

  REGRESS_MODEL:

    # model observations
    # pass "top_down_view" during evaluation to enable video
    visual_type: ["depth", "top_down_view"]

    # (optional) define modality ablations
    # modality to deactivate
    visual_strip: ["rgb"]
    # probability to deactivate modality
    visual_strip_proba: 0.75

    # action type to regress
    # ["unified_act", "sep_act"]
    regress_type: "unified_act"
    mode: "det"
    rnd_mode_n: 10
    pretrained: True

    # path to pretrained model
    # training automatically stores best model as 'best_vo.pth'
    pretrained_type: "regress_unified_best"
    all_pretrained_ckpt: {
      "regress_all_best": { 
        "all": "./train_log/final/vit/unique/vo_vit_b_dino_act_d_freeze/checkpoints/best_vo.pth",
        },
    }

    # fill in the corresponding configuration from the training configuration of the pre-trained VO model below

    name: "vo_transformer_act_embed"
    visual_backbone: "base" 
    train_backbone: False
    pretrain_backbone: 'dino'
    cls_action: True
    hidden_size: 512
    custom_model_path: 'pretrained'
    discretize_depth: "none"
    discretized_depth_channels: 0
    top_down_center_crop: False
    dropout_p: 0.
