{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123969c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import types\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5dbf2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vits16 = timm.create_model(\"vit_small_patch16_384\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd5c2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/memmelma/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "vits16dino = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d703d83c",
   "metadata": {},
   "source": [
    "Override functions of timm ViT. Remove asserts in ~/anaconda3/envs/iprl/lib/python3.7/site-packages/timm/models/layers/patch_embed.py which keeps us from using different image resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb1cc23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "def interpolate_pos_encoding(self, x, w, h):    \n",
    "    npatch = x.shape[1] - 1\n",
    "    N = self.pos_embed.shape[1] - 1\n",
    "    if npatch == N and w == h:\n",
    "        return self.pos_embed\n",
    "    class_pos_embed = self.pos_embed[:, 0]\n",
    "    patch_pos_embed = self.pos_embed[:, 1:]\n",
    "    dim = x.shape[-1]\n",
    "    \n",
    "    w0 = w // self.patch_embed.patch_size[0]\n",
    "    h0 = h // self.patch_embed.patch_size[0]\n",
    "    # we add a small number to avoid floating point error in the interpolation\n",
    "    # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "    w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "    patch_pos_embed = nn.functional.interpolate(\n",
    "        patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "        scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "        mode='bicubic',\n",
    "    )\n",
    "    assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
    "    patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "    return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "def prepare_tokens(self, x):\n",
    "    B, nc, w, h = x.shape\n",
    "    x = self.patch_embed(x)  # patch linear embedding\n",
    "\n",
    "    # add the [CLS] token to the embed patch tokens\n",
    "    cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "    x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "    # add positional encoding to each token\n",
    "    x = x + self.interpolate_pos_encoding(x, w, h)\n",
    "\n",
    "    return self.pos_drop(x)\n",
    "\n",
    "def forward(self, x):\n",
    "    x = self.prepare_tokens(x)\n",
    "    for blk in self.blocks:\n",
    "        x = blk(x)\n",
    "    x = self.norm(x)\n",
    "    return x[:, 0]\n",
    "\n",
    "vits16.interpolate_pos_encoding = types.MethodType(interpolate_pos_encoding, vits16)\n",
    "vits16.prepare_tokens = types.MethodType(prepare_tokens, vits16)\n",
    "vits16.forward = types.MethodType(forward, vits16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1089a712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/memmelma/anaconda3/envs/iprl/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "/home/memmelma/anaconda3/envs/iprl/lib/python3.7/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success vit 192,192 torch.Size([4, 384])\n",
      "success vit 192,384 torch.Size([4, 384])\n",
      "success vit 192,768 torch.Size([4, 384])\n",
      "success vit 384,192 torch.Size([4, 384])\n",
      "success vit 384,384 torch.Size([4, 384])\n",
      "success vit 384,768 torch.Size([4, 384])\n",
      "success vit 768,192 torch.Size([4, 384])\n",
      "success vit 768,384 torch.Size([4, 384])\n",
      "success vit 768,768 torch.Size([4, 384])\n"
     ]
    }
   ],
   "source": [
    "for s in list(itertools.product([192,384,768],[192,384,768])):\n",
    "    x = torch.rand((4,3,s[0],s[1]))\n",
    "    \n",
    "    try:\n",
    "        print(f'success vit {s[0]},{s[1]} {vits16.forward(x).shape}')\n",
    "    except:\n",
    "        print(f'failed vit {s[0]},{s[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebca17d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success dino 192,192 torch.Size([4, 384])\n",
      "success dino 192,384 torch.Size([4, 384])\n",
      "success dino 192,768 torch.Size([4, 384])\n",
      "success dino 384,192 torch.Size([4, 384])\n",
      "success dino 384,384 torch.Size([4, 384])\n",
      "success dino 384,768 torch.Size([4, 384])\n",
      "success dino 768,192 torch.Size([4, 384])\n",
      "success dino 768,384 torch.Size([4, 384])\n",
      "success dino 768,768 torch.Size([4, 384])\n"
     ]
    }
   ],
   "source": [
    "for s in list(itertools.product([192,384,768],[192,384,768])):\n",
    "    x = torch.rand((4,3,s[0],s[1]))\n",
    "    try:\n",
    "        print(f'success dino {s[0]},{s[1]} {vits16dino.forward(x).shape}')\n",
    "    except:\n",
    "        print(f'failed dino {s[0]},{s[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
