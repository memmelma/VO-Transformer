{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184bc6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pointnav_vo.vo import VisualOdometryTransformerActEmbed\n",
    "from pointnav_vo.config.vo_config.default import get_config as get_vo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce5a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_load_path = \"/datasets/home/memmel/PointNav-VO/img_attn_single\"\n",
    "images = [\"forward.png\", \"left.png\", \"right.png\"]\n",
    "actions = [1,2,3]\n",
    "\n",
    "config_path = \"/datasets/home/memmel/PointNav-VO/configs/vo/vit_baselines/\"\n",
    "configs = [\n",
    "    \"vo_vit_b_dino_act_rgbd.yaml\",\n",
    "    \"vo_vit_b_dino_act_rgbd_freeze.yaml\",\n",
    "    \"vo_vit_b_in21k_act_rgbd.yaml\",\n",
    "    \"vo_vit_b_in21k_act_rgbd_freeze.yaml\",\n",
    "    \"vo_vit_b_mmae_act_rgbd.yaml\",\n",
    "    \"vo_vit_b_mmae_act_rgbd_freeze.yaml\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2431af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config, model_load_path):\n",
    "    \n",
    "    model = VisualOdometryTransformerActEmbed(\n",
    "        observation_space=config.VO.MODEL.visual_type,\n",
    "        observation_size=(config.VO.VIS_SIZE_W, config.VO.VIS_SIZE_H),\n",
    "        hidden_size=config.VO.MODEL.hidden_size,\n",
    "        backbone=config.VO.MODEL.visual_backbone,\n",
    "        normalize_visual_inputs=True,\n",
    "        output_dim=3,\n",
    "        dropout_p=config.VO.MODEL.dropout_p,\n",
    "        discretized_depth_channels=0,\n",
    "        top_down_view_pair_channel=0,\n",
    "        cls_action=config.VO.MODEL.cls_action,\n",
    "        train_backbone=config.VO.MODEL.train_backbone,\n",
    "        pretrain_backbone=config.VO.MODEL.pretrain_backbone,\n",
    "        custom_model_path=config.VO.MODEL.custom_model_path,\n",
    "        depth_aux_loss=bool(config.VO.TRAIN.depth_aux_loss),\n",
    "    )\n",
    "    \n",
    "    checkpoint = torch.load(model_load_path, map_location=torch.device('cpu'))\n",
    "\n",
    "    def convert_dataparallel_weights(weights):\n",
    "        converted_weights = {}\n",
    "        keys = weights.keys()\n",
    "        for key in keys:\n",
    "            if 'vit.cls_token' in key:\n",
    "                continue\n",
    "            new_key = key.split(\"module.\")[-1]\n",
    "            converted_weights[new_key] = weights[key]\n",
    "        return converted_weights\n",
    "\n",
    "    model_state = convert_dataparallel_weights(checkpoint['model_states'][-1])\n",
    "    model.load_state_dict(model_state, strict=False)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model = load_model(config, model_load_path)\n",
    "\n",
    "# obs_size = model.obs_size\n",
    "# obs_size_single = model.obs_size_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "657d111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_input(image_load_path, action):\n",
    "    img_original = plt.imread(image_load_path)\n",
    "    rgb = torch.tensor(np.uint8(img_original[:img_original.shape[0]//2,:,:3] * 255))\n",
    "    rgb = torch.cat((rgb[:rgb.shape[0]//2],rgb[rgb.shape[0]//2:]),dim=2).unsqueeze(0)\n",
    "    depth = torch.tensor(img_original[img_original.shape[0]//2:,:,0]).unsqueeze(-1)\n",
    "    depth = torch.cat((depth[:depth.shape[0]//2],depth[depth.shape[0]//2:]),dim=2).unsqueeze(0)\n",
    "\n",
    "    batch_pairs = {}\n",
    "    batch_pairs[\"rgb\"] = rgb\n",
    "    batch_pairs[\"depth\"] = depth\n",
    "    batch_pairs[\"actions\"] = action\n",
    "    \n",
    "    return batch_pairs\n",
    "\n",
    "# action = torch.tensor([1])\n",
    "# batch_pairs = pre_process_input(image_load_path, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f2c65d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.observation_strip = [\"rgb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b79f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_img(batch_pairs, config, obs_size, obs_size_single):\n",
    "    rgb_check = \"rgb\" in config.VO.MODEL.visual_type\n",
    "    # dont visualize depth when aux depth loss is used\n",
    "    depth_check = \"depth\" in config.VO.MODEL.visual_type and not config.VO.TRAIN.depth_aux_loss\n",
    "\n",
    "    if rgb_check:\n",
    "        plot_idx = torch.randint(0, batch_pairs[\"rgb\"].shape[0], (1,1)).item()\n",
    "    elif depth_check:\n",
    "        plot_idx = torch.randint(0, batch_pairs[\"depth\"].shape[0], (1,1)).item()\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if rgb_check:\n",
    "        rgb = batch_pairs[\"rgb\"][plot_idx].unsqueeze(0)\n",
    "        rgb = torch.cat((rgb[:,:,:,:rgb.shape[-1]//2], rgb[:,:,:,rgb.shape[-1]//2:]),dim=1)\n",
    "        rgb = rgb.permute(0,3,1,2).contiguous()\n",
    "        rgb = torch.nn.functional.interpolate(rgb, size=(obs_size_single[0]*2,obs_size_single[1]))\n",
    "        img = rgb\n",
    "        if config.VO.MODEL.visual_type.count(\"rgb\") == 2:\n",
    "            img = torch.cat((rgb,rgb),dim=2)\n",
    "    if depth_check:\n",
    "        depth =  batch_pairs[\"depth\"][plot_idx].unsqueeze(0)\n",
    "        depth = torch.cat((depth[:,:,:,:depth.shape[-1]//2], depth[:,:,:,depth.shape[-1]//2:]),dim=1)\n",
    "        depth = depth.permute(0,3,1,2).contiguous()\n",
    "        depth = torch.nn.functional.interpolate(depth, size=(obs_size_single[0]*2,obs_size_single[1]))\n",
    "        depth = depth.expand(-1, 3, -1, -1) * 255.\n",
    "        img = depth\n",
    "        if config.VO.MODEL.visual_type.count(\"depth\") == 2:\n",
    "            img = torch.cat((depth,depth),dim=2)\n",
    "\n",
    "    if rgb_check and depth_check:\n",
    "        img = torch.cat((rgb,depth),dim=2)\n",
    "\n",
    "    # log plain image\n",
    "    img = img.cpu().numpy().squeeze()\n",
    "    img = np.uint8(img).transpose(1,2,0)\n",
    "\n",
    "    return img, plot_idx\n",
    "    \n",
    "# plot_img, plot_idx = pre_process_img(batch_pairs, config, obs_size, obs_size_single)\n",
    "# plt.imshow(plot_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40a327aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention(model, batch_pairs, action, reduce=\"max\", color_map=cm.inferno):\n",
    "    \n",
    "    batch_pairs[\"actions\"] = action\n",
    "    features, attn = model(batch_pairs, batch_pairs[\"actions\"], return_attention=True)\n",
    "    batch_pairs[\"self_attention\"] = attn\n",
    "\n",
    "    nh = batch_pairs[\"self_attention\"].shape[1] # number of head\n",
    "    # we keep only the output patch attention\n",
    "    if config.VO.MODEL.pretrain_backbone == 'mmae':\n",
    "        # last token is cls token\n",
    "        cls_token_idx = -1\n",
    "        attn = batch_pairs[\"self_attention\"][plot_idx, :, -1, :-1].reshape(nh, -1)\n",
    "\n",
    "    else:\n",
    "        # first token is cls token\n",
    "        cls_token_idx = 0\n",
    "        attn = batch_pairs[\"self_attention\"][plot_idx, :, 0, 1:].reshape(nh, -1)\n",
    "\n",
    "    path_size = 16\n",
    "    attn = attn.reshape(nh, obs_size[0]//path_size, obs_size[1]//path_size)\n",
    "    attn = torch.nn.functional.interpolate(attn.unsqueeze(0), scale_factor=path_size, mode=\"nearest\")[0]\n",
    "\n",
    "    attn = attn.detach().cpu().numpy()\n",
    "    \n",
    "    if reduce == \"max\":\n",
    "        attn_agg = np.max(attn, axis=0)\n",
    "    elif reduce == \"min\":\n",
    "        attn_agg = np.min(attn, axis=0)\n",
    "    else:\n",
    "        attn_agg = np.mean(attn, axis=0)\n",
    "    \n",
    "    \n",
    "    attn_agg = np.uint8(255*color_map(attn_agg/ attn_agg.max())[:,:,:-1])\n",
    "    \n",
    "    return attn_agg\n",
    "        \n",
    "# attn_agg_max = get_attention(action)\n",
    "# plt.imshow(attn_agg_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c8fb399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_attention(img, attn, action, img_name, obs_size_single, save_path):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for i in range(attn.shape[0]//obs_size_single[0]):\n",
    "        attn_single = attn[i*obs_size_single[0]:(i+1)*obs_size_single[0]]\n",
    "        plt.imsave(os.path.join(save_path,f'{img_name}_attn_act_{action.item()}_{i}.png'), attn_single)\n",
    "        img_single = img[i*obs_size_single[0]:(i+1)*obs_size_single[0]]\n",
    "        plt.imsave(os.path.join(save_path,f'{img_name}_img_act_{action.item()}_{i}.png'), img_single)\n",
    "        overlay_single = cv2.addWeighted(attn_single, 0.8, img_single, 0.6, 0.0)\n",
    "        plt.imsave(os.path.join(save_path,f'{img_name}_overlay_act_{action.item()}_{i}.png'), overlay_single)\n",
    "        \n",
    "# plot_img_attention(plot_img, attn_agg_max, action, obs_size_single, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bd8e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cfg in configs:\n",
    "    \n",
    "    config_yaml = os.path.join(config_path, cfg)\n",
    "    config = get_vo_config(config_yaml, [])\n",
    "    \n",
    "    exp = config_yaml.split('/')[-1].split('.')[0]\n",
    "    model_load_path = os.path.join(\"train_log/final/vit/unique/\", exp, \"checkpoints/best_vo.pth\")\n",
    "    \n",
    "    save_path = os.path.join(\"img_attn_single\", exp)\n",
    "    \n",
    "    model = load_model(config, model_load_path)\n",
    "    obs_size = model.obs_size\n",
    "    obs_size_single = model.obs_size_single\n",
    "    \n",
    "    for img, act in zip(images, actions):\n",
    "        act = torch.tensor(act)\n",
    "        img_name = img.split('.')[0]\n",
    "        \n",
    "        image_path = os.path.join(image_load_path, img)\n",
    "        batch_pairs = pre_process_input(image_path, act)\n",
    "        plot_img, plot_idx = pre_process_img(batch_pairs, config, obs_size, obs_size_single)\n",
    "        attn_agg_max = get_attention(model, batch_pairs, act)\n",
    "        plot_img_attention(plot_img, attn_agg_max, act, img_name, obs_size_single, save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76559f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\"right.png\"]\n",
    "actions = [1,2,3]\n",
    "\n",
    "configs = [\"vo_vit_b_mmae_act_rgbd.yaml\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2633bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cfg in configs:\n",
    "    \n",
    "    config_yaml = os.path.join(config_path, cfg)\n",
    "    config = get_vo_config(config_yaml, [])\n",
    "    \n",
    "    exp = config_yaml.split('/')[-1].split('.')[0]\n",
    "    model_load_path = os.path.join(\"train_log/final/vit/unique/\", exp, \"checkpoints/best_vo.pth\")\n",
    "    \n",
    "    save_path = os.path.join(\"img_attn_single\", exp)\n",
    "    \n",
    "    model = load_model(config, model_load_path)\n",
    "    obs_size = model.obs_size\n",
    "    obs_size_single = model.obs_size_single\n",
    "    \n",
    "    for img in images:\n",
    "        for act in actions:\n",
    "            act = torch.tensor(act)\n",
    "            img_name = img.split('.')[0]\n",
    "\n",
    "            image_path = os.path.join(image_load_path, img)\n",
    "            batch_pairs = pre_process_input(image_path, act)\n",
    "            plot_img, plot_idx = pre_process_img(batch_pairs, config, obs_size, obs_size_single)\n",
    "            attn_agg_max = get_attention(model, batch_pairs, act)\n",
    "            plot_img_attention(plot_img, attn_agg_max, act, img_name, obs_size_single, save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3de0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
